{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import gc\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import re\n",
    "\n",
    "def select_model():\n",
    "    \"\"\"Prompt user to choose a model\"\"\"\n",
    "    model_name = input(\"Enter Hugging Face model name (e.g., 'gpt2', 'EleutherAI/gpt-neo-125M'): \")\n",
    "    return model_name\n",
    "\n",
    "def get_total_size(model):\n",
    "    \"\"\"Calculate total model parameter size in GB\"\"\"\n",
    "    total_size_bytes = sum(p.element_size() * p.nelement() for p in model.parameters())\n",
    "    return total_size_bytes / (1024 ** 3)\n",
    "\n",
    "def find_attention_layers(model) -> List[Tuple[str, nn.Module]]:\n",
    "    \"\"\"\n",
    "    Find all attention layers in the model regardless of architecture\n",
    "    Returns list of (layer_name, layer_module) tuples\n",
    "    \"\"\"\n",
    "    attention_layers = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        # Look for common attention layer patterns\n",
    "        if any(pattern in name.lower() for pattern in ['attn', 'attention', 'self_attn']):\n",
    "            # Check if it's a linear layer that could be attention weights\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Common patterns for attention weight matrices\n",
    "                if any(suffix in name for suffix in ['q_proj', 'k_proj', 'v_proj', 'qkv', 'c_attn', 'query', 'key', 'value']):\n",
    "                    attention_layers.append((name, module))\n",
    "    \n",
    "    return attention_layers\n",
    "\n",
    "def get_model_architecture_info(model) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract architecture-specific information from the model\n",
    "    \"\"\"\n",
    "    config = model.config\n",
    "    arch_info = {\n",
    "        'num_attention_heads': getattr(config, 'num_attention_heads', getattr(config, 'n_head', 12)),\n",
    "        'hidden_size': getattr(config, 'hidden_size', getattr(config, 'n_embd', 768)),\n",
    "        'num_layers': getattr(config, 'num_hidden_layers', getattr(config, 'n_layer', 12)),\n",
    "        'model_type': getattr(config, 'model_type', 'unknown')\n",
    "    }\n",
    "    \n",
    "    arch_info['head_dim'] = arch_info['hidden_size'] // arch_info['num_attention_heads']\n",
    "    return arch_info\n",
    "\n",
    "def compute_head_importance_universal(model, arch_info: Dict) -> Dict[int, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute head importance for any model architecture using L2 norm\n",
    "    \"\"\"\n",
    "    head_importance = {}\n",
    "    attention_layers = find_attention_layers(model)\n",
    "    \n",
    "    if not attention_layers:\n",
    "        raise NotImplementedError(\"No attention layers found in this model\")\n",
    "    \n",
    "    # Group attention layers by layer index\n",
    "    layer_groups = {}\n",
    "    for name, module in attention_layers:\n",
    "        # Extract layer number from name (e.g., \"layer.0.attention.query\" -> 0)\n",
    "        layer_match = re.search(r'(?:layer|h|block)\\.(\\d+)', name)\n",
    "        if layer_match:\n",
    "            layer_idx = int(layer_match.group(1))\n",
    "            if layer_idx not in layer_groups:\n",
    "                layer_groups[layer_idx] = []\n",
    "            layer_groups[layer_idx].append((name, module))\n",
    "    \n",
    "    num_heads = arch_info['num_attention_heads']\n",
    "    head_dim = arch_info['head_dim']\n",
    "    \n",
    "    for layer_idx, layer_modules in layer_groups.items():\n",
    "        layer_importance = torch.zeros(num_heads)\n",
    "        \n",
    "        for name, module in layer_modules:\n",
    "            weight = module.weight.data\n",
    "            \n",
    "            # Handle different attention weight organizations\n",
    "            if 'qkv' in name or 'c_attn' in name:\n",
    "                # Combined QKV matrix (like GPT-2)\n",
    "                if weight.shape[0] == 3 * arch_info['hidden_size']:\n",
    "                    # Reshape to [3, num_heads, head_dim, hidden_size]\n",
    "                    qkv_weight = weight.view(3, num_heads, head_dim, -1)\n",
    "                    # Compute L2 norm for each head across Q, K, V\n",
    "                    head_norms = qkv_weight.norm(dim=(0, 2, 3))\n",
    "                    layer_importance += head_norms.cpu()\n",
    "            else:\n",
    "                # Separate Q, K, V matrices (like BERT, LLaMA)\n",
    "                if weight.shape[0] == arch_info['hidden_size']:\n",
    "                    # Reshape to [num_heads, head_dim, hidden_size]\n",
    "                    head_weight = weight.view(num_heads, head_dim, -1)\n",
    "                    # Compute L2 norm for each head\n",
    "                    head_norms = head_weight.norm(dim=(1, 2))\n",
    "                    layer_importance += head_norms.cpu()\n",
    "        \n",
    "        if layer_importance.sum() > 0:\n",
    "            head_importance[layer_idx] = layer_importance\n",
    "    \n",
    "    return head_importance\n",
    "\n",
    "def prune_attention_weights_universal(model, head_importance: Dict[int, torch.Tensor], \n",
    "                                    prune_percent: float, arch_info: Dict) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Universal attention pruning by zeroing out least important heads\n",
    "    \"\"\"\n",
    "    heads_to_prune = {}\n",
    "    original_heads = arch_info['num_attention_heads']\n",
    "    total_heads_pruned = 0\n",
    "    \n",
    "    # Determine which heads to prune\n",
    "    for layer_idx, importance in head_importance.items():\n",
    "        num_heads = len(importance)\n",
    "        n_prune = max(1, int(prune_percent * num_heads))\n",
    "        n_prune = min(n_prune, num_heads - 1)  # Leave at least 1 head\n",
    "        \n",
    "        # Get indices of least important heads\n",
    "        prune_idxs = torch.topk(importance, k=n_prune, largest=False).indices.tolist()\n",
    "        heads_to_prune[layer_idx] = prune_idxs\n",
    "        total_heads_pruned += n_prune\n",
    "        print(f\"Layer {layer_idx}: pruning {n_prune}/{num_heads} heads (indices: {prune_idxs})\")\n",
    "    \n",
    "    # Apply pruning by zeroing weights\n",
    "    attention_layers = find_attention_layers(model)\n",
    "    head_dim = arch_info['head_dim']\n",
    "    \n",
    "    for name, module in attention_layers:\n",
    "        # Extract layer number\n",
    "        layer_match = re.search(r'(?:layer|h|block)\\.(\\d+)', name)\n",
    "        if layer_match:\n",
    "            layer_idx = int(layer_match.group(1))\n",
    "            if layer_idx in heads_to_prune:\n",
    "                prune_idxs = heads_to_prune[layer_idx]\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    weight = module.weight.data\n",
    "                    \n",
    "                    if 'qkv' in name or 'c_attn' in name:\n",
    "                        # Combined QKV matrix\n",
    "                        if weight.shape[0] == 3 * arch_info['hidden_size']:\n",
    "                            # Zero out the weights for pruned heads\n",
    "                            for head_idx in prune_idxs:\n",
    "                                start_q = head_idx * head_dim\n",
    "                                end_q = (head_idx + 1) * head_dim\n",
    "                                start_k = arch_info['hidden_size'] + head_idx * head_dim\n",
    "                                end_k = arch_info['hidden_size'] + (head_idx + 1) * head_dim\n",
    "                                start_v = 2 * arch_info['hidden_size'] + head_idx * head_dim\n",
    "                                end_v = 2 * arch_info['hidden_size'] + (head_idx + 1) * head_dim\n",
    "                                \n",
    "                                weight[start_q:end_q, :] = 0\n",
    "                                weight[start_k:end_k, :] = 0\n",
    "                                weight[start_v:end_v, :] = 0\n",
    "                    else:\n",
    "                        # Separate Q, K, V matrices\n",
    "                        if weight.shape[0] == arch_info['hidden_size']:\n",
    "                            for head_idx in prune_idxs:\n",
    "                                start_idx = head_idx * head_dim\n",
    "                                end_idx = (head_idx + 1) * head_dim\n",
    "                                weight[start_idx:end_idx, :] = 0\n",
    "                    \n",
    "                    # Also zero bias if present\n",
    "                    if hasattr(module, 'bias') and module.bias is not None:\n",
    "                        bias = module.bias.data\n",
    "                        if 'qkv' in name or 'c_attn' in name:\n",
    "                            if bias.shape[0] == 3 * arch_info['hidden_size']:\n",
    "                                for head_idx in prune_idxs:\n",
    "                                    start_q = head_idx * head_dim\n",
    "                                    end_q = (head_idx + 1) * head_dim\n",
    "                                    start_k = arch_info['hidden_size'] + head_idx * head_dim\n",
    "                                    end_k = arch_info['hidden_size'] + (head_idx + 1) * head_dim\n",
    "                                    start_v = 2 * arch_info['hidden_size'] + head_idx * head_dim\n",
    "                                    end_v = 2 * arch_info['hidden_size'] + (head_idx + 1) * head_dim\n",
    "                                    \n",
    "                                    bias[start_q:end_q] = 0\n",
    "                                    bias[start_k:end_k] = 0\n",
    "                                    bias[start_v:end_v] = 0\n",
    "                        else:\n",
    "                            if bias.shape[0] == arch_info['hidden_size']:\n",
    "                                for head_idx in prune_idxs:\n",
    "                                    start_idx = head_idx * head_dim\n",
    "                                    end_idx = (head_idx + 1) * head_dim\n",
    "                                    bias[start_idx:end_idx] = 0\n",
    "    \n",
    "    return total_heads_pruned, original_heads\n",
    "\n",
    "def prune_model_universal(model, prune_percent: float = 0.1):\n",
    "    \"\"\"\n",
    "    Universal model pruning function that works with any architecture\n",
    "    \"\"\"\n",
    "    print(\"Analyzing model architecture...\")\n",
    "    arch_info = get_model_architecture_info(model)\n",
    "    print(f\"Model type: {arch_info['model_type']}\")\n",
    "    print(f\"Number of layers: {arch_info['num_layers']}\")\n",
    "    print(f\"Number of attention heads: {arch_info['num_attention_heads']}\")\n",
    "    print(f\"Head dimension: {arch_info['head_dim']}\")\n",
    "    \n",
    "    print(\"\\nComputing head importance...\")\n",
    "    head_importance = compute_head_importance_universal(model, arch_info)\n",
    "    \n",
    "    print(f\"\\nPruning {prune_percent*100:.0f}% of heads per layer...\")\n",
    "    total_heads_pruned, original_heads = prune_attention_weights_universal(\n",
    "        model, head_importance, prune_percent, arch_info\n",
    "    )\n",
    "    \n",
    "    return total_heads_pruned, original_heads, arch_info\n",
    "\n",
    "# ---- Main execution ----\n",
    "try:\n",
    "    model_name = select_model()\n",
    "    print(f\"Loading model {model_name}...\")\n",
    "    \n",
    "    # Load tokenizer and model with error handling\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load model/tokenizer: {str(e)}\")\n",
    "    \n",
    "    # Device management\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    print(f\"Model loaded on {device}\")\n",
    "    \n",
    "    # Get original size\n",
    "    original_size = get_total_size(model)\n",
    "    print(f\"\\nOriginal size: {original_size:.2f} GB\")\n",
    "    \n",
    "    # Pruning parameters\n",
    "    PRUNE_PERCENT = 0.20\n",
    "    \n",
    "    # Apply universal pruning\n",
    "    total_heads_pruned, original_heads, arch_info = prune_model_universal(model, PRUNE_PERCENT)\n",
    "    \n",
    "    # Calculate final size and statistics\n",
    "    pruned_size = get_total_size(model)\n",
    "    print(f\"\\nPruned size: {pruned_size:.2f} GB\")\n",
    "    print(f\"Reduction: {original_size - pruned_size:.2f} GB ({(1 - pruned_size/original_size)*100:.1f}%)\")\n",
    "    print(f\"Total heads pruned: {total_heads_pruned}\")\n",
    "    print(f\"Pruning percentage: {PRUNE_PERCENT*100:.0f}% per layer\")\n",
    "    print(f\"Original heads per layer: {original_heads}\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save the pruned model\n",
    "    safe_model_name = model_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "    output_dir = f\"./pruned_model_{safe_model_name}\"\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        model.save_pretrained(output_dir, safe_serialization=True)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"\\nPruned model saved to: {output_dir}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to save model: {str(e)}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOperation cancelled by user\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
