{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e545da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Tuple, Callable, Optional\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "from typing import Dict, Tuple, Callable, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DistillationTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        teacher_model: nn.Module,\n",
    "        student_model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        loss_fn: Callable,\n",
    "        device: torch.device,\n",
    "        config: Dict\n",
    "    ):\n",
    "        self.teacher = teacher_model\n",
    "        self.student = student_model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        \n",
    "        # Настройка логирования\n",
    "        self.logger = logger\n",
    "        self.logger.setLevel(config.get('log_level', logging.INFO))\n",
    "        self.metrics = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    def _run_epoch(self, epoch: int) -> float:\n",
    "        self.student.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            # Forward pass учителя\n",
    "            with torch.no_grad():\n",
    "                teacher_output = self.teacher(data)\n",
    "            \n",
    "            # Forward pass студента\n",
    "            student_output = self.student(data)\n",
    "            \n",
    "            # Вычисление потерь\n",
    "            loss = self.loss_fn(\n",
    "                student_output=student_output,\n",
    "                teacher_output=teacher_output,\n",
    "                target=target,\n",
    "                config=self.config\n",
    "            )\n",
    "            \n",
    "            # Оптимизация\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % self.config.get('log_interval', 100) == 0:\n",
    "                self.logger.info(\n",
    "                    f\"Train Epoch: {epoch} [{batch_idx}/{len(self.train_loader)}] \"\n",
    "                    f\"Loss: {loss.item():.6f}\"\n",
    "                )\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        self.metrics['train_loss'].append(avg_loss)\n",
    "        return avg_loss\n",
    "\n",
    "    def _validate(self) -> Tuple[float, float]:\n",
    "        self.student.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in self.val_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.student(data)\n",
    "                \n",
    "                # Потери на валидации (только student loss)\n",
    "                val_loss += F.cross_entropy(output, target).item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "        \n",
    "        val_loss /= len(self.val_loader)\n",
    "        accuracy = 100. * correct / len(self.val_loader.dataset)\n",
    "        \n",
    "        self.metrics['val_loss'].append(val_loss)\n",
    "        self.metrics['val_acc'].append(accuracy)\n",
    "        return val_loss, accuracy\n",
    "\n",
    "    def run(self) -> Dict:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(1, self.config['epochs'] + 1):\n",
    "            train_loss = self._run_epoch(epoch)\n",
    "            val_loss, val_acc = self._validate()\n",
    "            \n",
    "            self.logger.info(\n",
    "                f\"\\nEpoch: {epoch} | Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\\n\"\n",
    "            )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        self.logger.info(f\"Training completed in {training_time:.2f} seconds\")\n",
    "        return self.metrics\n",
    "\n",
    "\n",
    "def distillation_loss(\n",
    "    student_output: torch.Tensor,\n",
    "    teacher_output: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    config: Dict\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Комбинированная функция потерь для дистилляции:\n",
    "    loss = α * soft_loss + (1 - α) * hard_loss\n",
    "    \n",
    "    Args:\n",
    "        student_output: Выход модели-студента (логиты)\n",
    "        teacher_output: Выход модели-учителя (логиты)\n",
    "        target: Истинные метки\n",
    "        config: Конфигурация с параметрами\n",
    "        \n",
    "    Returns:\n",
    "        Комбинированный лосс\n",
    "    \"\"\"\n",
    "    T = config.get('temperature', 5.0)\n",
    "    alpha = config.get('alpha', 0.7)\n",
    "    \n",
    "    # Soft targets (дистилляция)\n",
    "    soft_loss = F.kl_div(\n",
    "        input=F.log_softmax(student_output / T, dim=1),\n",
    "        target=F.softmax(teacher_output / T, dim=1),\n",
    "        reduction='batchmean'\n",
    "    ) * (T * T)  # Scale by T^2\n",
    "    \n",
    "    # Hard targets (обычная кросс-энтропия)\n",
    "    hard_loss = F.cross_entropy(student_output, target)\n",
    "    \n",
    "    return alpha * soft_loss + (1 - alpha) * hard_loss\n",
    "\n",
    "\n",
    "def distill(\n",
    "    teacher_model: nn.Module,\n",
    "    student_model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    config: Dict,\n",
    "    loss_fn: Optional[Callable] = distillation_loss,\n",
    "    device: Optional[torch.device] = None\n",
    ") -> Tuple[nn.Module, Dict]:\n",
    "    \"\"\"\n",
    "    Основная функция дистилляции знаний.\n",
    "    \n",
    "    Args:\n",
    "        teacher_model: Обученная модель-учитель\n",
    "        student_model: Модель-студент для обучения\n",
    "        train_loader: DataLoader для обучения\n",
    "        val_loader: DataLoader для валидации\n",
    "        optimizer: Оптимизатор для студента\n",
    "        config: Конфигурационный словарь\n",
    "        loss_fn: Функция потерь (по умолчанию distillation_loss)\n",
    "        device: Устройство для вычислений\n",
    "    \n",
    "    Returns:\n",
    "        Обученная модель-студент и метрики\n",
    "    \"\"\"\n",
    "    # Настройка устройства\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    teacher_model.to(device)\n",
    "    student_model.to(device)\n",
    "    teacher_model.eval()\n",
    "    \n",
    "    # Инициализация тренера\n",
    "    trainer = DistillationTrainer(\n",
    "        teacher_model=teacher_model,\n",
    "        student_model=student_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        device=device,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Запуск обучения\n",
    "    metrics = trainer.run()\n",
    "    return student_model, metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
