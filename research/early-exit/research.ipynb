{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0e52a8",
   "metadata": {},
   "source": [
    "# Early Exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372cfe58",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a238143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import\n",
    "\n",
    "class EarlyExitLM(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, threshold: float = 1.0, min_layers: int = 1):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        # Для удобства вытащим список трансформер-блоков и head\n",
    "        self.blocks = base_model.transformer.h\n",
    "        self.wte    = base_model.transformer.wte\n",
    "        self.wpe    = base_model.transformer.wpe\n",
    "        self.lm_head= base_model.lm_head\n",
    "        self.threshold = threshold\n",
    "        self.min_layers = min_layers\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # эмбеддинги + позиционные\n",
    "        device = input_ids.device\n",
    "        seq_len = input_ids.size(-1)\n",
    "        hidden = self.wte(input_ids) + self.wpe(torch.arange(seq_len, device=device))\n",
    "        \n",
    "        exit_layer = len(self.blocks)\n",
    "        logits = None\n",
    "\n",
    "        # по-блочно\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            hidden = block(hidden, attn_mask=attention_mask)[0]\n",
    "            \n",
    "            # попробуем выйти, начиная с min_layers\n",
    "            if i + 1 >= self.min_layers:\n",
    "                lm_logits = self.lm_head(hidden)\n",
    "                probs = torch.softmax(lm_logits[:, -1, :], dim=-1)\n",
    "                entropy = - (probs * torch.log(probs + 1e-12)).sum(dim=-1).mean().item()\n",
    "                if entropy < self.threshold:\n",
    "                    logits = lm_logits\n",
    "                    exit_layer = i + 1\n",
    "                    break\n",
    "\n",
    "        # если выхода не случилось — считаем на последнем состоянии\n",
    "        if logits is None:\n",
    "            logits = self.lm_head(hidden)\n",
    "\n",
    "        return logits, exit_layer\n",
    "\n",
    "def optimize_model_with_early_exit(\n",
    "    base_model: nn.Module,\n",
    "    threshold: float = 1.0,\n",
    "    min_layers: int = 1\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Оборачивает любую causal-LM модель в EarlyExitLM с заданными параметрами.\n",
    "    \"\"\"\n",
    "    return EarlyExitLM(base_model, threshold=threshold, min_layers=min_layers)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Загрузка модели с вашими опциями\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"openai-community/gpt2\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"sdpa\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # 2) Оптимизация\n",
    "    early_model = optimize_model_with_early_exit(model, threshold=2.5, min_layers=2).eval()\n",
    "\n",
    "    # 3) Тест на простом примере\n",
    "    import time\n",
    "    text = \"The quick brown fox jumps over\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # baseline\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs).logits\n",
    "    print(\"Baseline:\", time.time() - start)\n",
    "\n",
    "    # early-exit\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        logits, exit_layer = early_model(inputs.input_ids, attention_mask=inputs.attention_mask)\n",
    "    print(\"Early-exit:\", time.time() - start, \"Exited at layer\", exit_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
