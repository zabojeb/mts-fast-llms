#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–¢–µ—Å—Ç–æ–≤—ã–π –º–æ–¥—É–ª—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π

–≠—Ç–æ—Ç —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –º–æ–¥—É–ª—è distillation.py
–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import argparse
import sys
import os
from typing import Dict, Any, Tuple
import matplotlib.pyplot as plt
import pandas as pd
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config
from datasets import load_dataset
import warnings
warnings.filterwarnings('ignore')

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–µ–≥–æ –º–æ–¥—É–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏
from distillation import KnowledgeDistillation, DistillationConfig, create_optimizer


class SimpleTeacherModel(nn.Module):
    """–ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å —É—á–∏—Ç–µ–ª—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, input_size: int = 784, hidden_size: int = 512, num_classes: int = 10):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, num_classes)
        )
    
    def forward(self, x):
        return self.network(x.view(x.size(0), -1))


class SimpleStudentModel(nn.Module):
    """–ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å —Å—Ç—É–¥–µ–Ω—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, input_size: int = 784, hidden_size: int = 128, num_classes: int = 10):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, num_classes)
        )
    
    def forward(self, x):
        return self.network(x.view(x.size(0), -1))


def create_synthetic_data(num_samples: int = 1000, input_size: int = 784, num_classes: int = 10) -> Tuple[DataLoader, DataLoader]:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    X = torch.randn(num_samples, input_size)
    y = torch.randint(0, num_classes, (num_samples,))
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    split_idx = int(0.8 * num_samples)
    
    train_dataset = TensorDataset(X[:split_idx], y[:split_idx])
    test_dataset = TensorDataset(X[split_idx:], y[split_idx:])
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    return train_loader, test_loader


def test_basic_functionality():
    """–¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏"""
    print("\n=== –¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = DistillationConfig(
        temperature=4.0,
        alpha=0.7,
        beta=0.3,
        log_interval=10
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Å—Ç–∏–ª–ª—è—Ç–æ—Ä–∞
    distiller = KnowledgeDistillation(config)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    teacher = SimpleTeacherModel()
    student = SimpleStudentModel()
    
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —É—á–∏—Ç–µ–ª—è: {sum(p.numel() for p in teacher.parameters()):,}")
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ç—É–¥–µ–Ω—Ç–∞: {sum(p.numel() for p in student.parameters()):,}")
    
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π
    distiller.set_models(teacher, student)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    batch_size = 16
    inputs = torch.randn(batch_size, 784)
    targets = torch.randint(0, 10, (batch_size,))
    
    # –¢–µ—Å—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
    with torch.no_grad():
        teacher_logits = teacher(inputs)
        student_logits = student(inputs)
    
    losses = distiller.distillation_loss(student_logits, teacher_logits, targets)
    
    print(f"–û–±—â–∞—è –ø–æ—Ç–µ—Ä—è: {losses['total_loss'].item():.4f}")
    print(f"–ü–æ—Ç–µ—Ä—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏: {losses['distillation_loss'].item():.4f}")
    print(f"–ü–æ—Ç–µ—Ä—è —Å—Ç—É–¥–µ–Ω—Ç–∞: {losses['student_loss'].item():.4f}")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    metrics = distiller.get_metrics()
    print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {metrics['config']['compression_ratio']:.2f}x")
    
    print("‚úÖ –ë–∞–∑–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    return True


def test_training_process():
    """–¢–µ—Å—Ç –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    print("\n=== –¢–µ—Å—Ç –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    train_loader, test_loader = create_synthetic_data(num_samples=500)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    teacher = SimpleTeacherModel()
    student = SimpleStudentModel()
    
    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —É—á–∏—Ç–µ–ª—è (—Å–∏–º—É–ª—è—Ü–∏—è)
    print("–°–∏–º—É–ª—è—Ü–∏—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —É—á–∏—Ç–µ–ª—è...")
    teacher_optimizer = optim.Adam(teacher.parameters(), lr=0.001)
    teacher.train()
    
    for epoch in range(2):
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            teacher_optimizer.zero_grad()
            outputs = teacher(inputs)
            loss = nn.CrossEntropyLoss()(outputs, targets)
            loss.backward()
            teacher_optimizer.step()
            
            if batch_idx % 5 == 0:
                print(f"–£—á–∏—Ç–µ–ª—å - –≠–ø–æ—Ö–∞ {epoch+1}, –ë–∞—Ç—á {batch_idx}, –ü–æ—Ç–µ—Ä—è: {loss.item():.4f}")
    
    # –î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è
    config = DistillationConfig(
        temperature=3.0,
        alpha=0.8,
        beta=0.2,
        log_interval=5
    )
    
    distiller = KnowledgeDistillation(config)
    student_optimizer = optim.Adam(student.parameters(), lr=0.001)
    
    print("\n–ù–∞—á–∞–ª–æ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏...")
    trained_student = distiller.apply(
        student_model=student,
        teacher_model=teacher,
        train_loader=train_loader,
        optimizer=student_optimizer,
        num_epochs=2
    )
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
    final_metrics = distiller.get_metrics()
    print(f"\n–§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
    print(f"–°—Ä–µ–¥–Ω—è—è –æ–±—â–∞—è –ø–æ—Ç–µ—Ä—è: {final_metrics['metrics']['avg_total_loss']:.4f}")
    print(f"–°—Ä–µ–¥–Ω—è—è –ø–æ—Ç–µ—Ä—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏: {final_metrics['metrics']['avg_distillation_loss']:.4f}")
    print(f"–°—Ä–µ–¥–Ω—è—è –ø–æ—Ç–µ—Ä—è —Å—Ç—É–¥–µ–Ω—Ç–∞: {final_metrics['metrics']['avg_student_loss']:.4f}")
    print(f"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {final_metrics['metrics']['elapsed_time']:.2f} —Å–µ–∫")
    
    print("‚úÖ –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    return True


def test_gpt2_distillation(sample_size: int = 100):
    """–¢–µ—Å—Ç –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Å –º–æ–¥–µ–ª—è–º–∏ GPT-2"""
    print("\n=== –¢–µ—Å—Ç –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ GPT-2 ===")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
        tokenizer.pad_token = tokenizer.eos_token
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π GPT-2...")
        teacher_model = GPT2LMHeadModel.from_pretrained('gpt2-medium')
        student_model = GPT2LMHeadModel.from_pretrained('gpt2')
        
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —É—á–∏—Ç–µ–ª—è (GPT-2 medium): {sum(p.numel() for p in teacher_model.parameters()):,}")
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ç—É–¥–µ–Ω—Ç–∞ (GPT-2): {sum(p.numel() for p in student_model.parameters()):,}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        texts = [
            "The quick brown fox jumps over the lazy dog.",
            "Machine learning is a subset of artificial intelligence.",
            "Python is a popular programming language for data science.",
            "Knowledge distillation helps create smaller models.",
            "Natural language processing enables computers to understand text."
        ] * (sample_size // 5)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        max_length = 64
        encoded = tokenizer(
            texts[:sample_size], 
            padding=True, 
            truncation=True, 
            max_length=max_length, 
            return_tensors='pt'
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
        dataset = TensorDataset(encoded['input_ids'], encoded['input_ids'])  # targets = inputs –¥–ª—è LM
        dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏
        config = DistillationConfig(
            temperature=4.0,
            alpha=0.7,
            beta=0.3,
            log_interval=5,
            device='cpu'  # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Å—Ç–∏–ª–ª—è—Ç–æ—Ä–∞
        distiller = KnowledgeDistillation(config)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–∞
        optimizer = optim.AdamW(student_model.parameters(), lr=5e-5)
        
        # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        def lm_loss_fn(logits, targets):
            # –°–¥–≤–∏–≥–∞–µ–º targets –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = targets[..., 1:].contiguous()
            return nn.CrossEntropyLoss()(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        
        print("\n–ù–∞—á–∞–ª–æ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ GPT-2...")
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏
        trained_student = distiller.apply(
            student_model=student_model,
            teacher_model=teacher_model,
            train_loader=dataloader,
            optimizer=optimizer,
            num_epochs=1,
            student_loss_fn=lm_loss_fn
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        save_path = "distilled_gpt2_student.pt"
        distiller.save_student_model(save_path)
        print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {save_path}")
        
        print("‚úÖ –î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è GPT-2 –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ GPT-2: {e}")
        print("–í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
        return False


def test_pokemon_dataset(sample_size: int = 200):
    """–¢–µ—Å—Ç —Å –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö –æ –ø–æ–∫–µ–º–æ–Ω–∞—Ö"""
    print("\n=== –¢–µ—Å—Ç —Å –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö –æ –ø–æ–∫–µ–º–æ–Ω–∞—Ö ===")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –æ –ø–æ–∫–µ–º–æ–Ω–∞—Ö...")
        dataset = load_dataset("lamini/pokemon-bleu", split="train")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏
        if len(dataset) > sample_size:
            dataset = dataset.select(range(sample_size))
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        texts = [item['text'] for item in dataset]
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
        tokenizer.pad_token = tokenizer.eos_token
        
        encoded = tokenizer(
            texts, 
            padding=True, 
            truncation=True, 
            max_length=128, 
            return_tensors='pt'
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
        dataset_tensor = TensorDataset(encoded['input_ids'], encoded['input_ids'])
        dataloader = DataLoader(dataset_tensor, batch_size=4, shuffle=True)
        
        # –ú–æ–¥–µ–ª–∏
        teacher_model = GPT2LMHeadModel.from_pretrained('gpt2')
        student_config = GPT2Config.from_pretrained('gpt2')
        student_config.n_layer = 6  # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤
        student_config.n_head = 8   # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
        student_model = GPT2LMHeadModel(student_config)
        
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —É—á–∏—Ç–µ–ª—è: {sum(p.numel() for p in teacher_model.parameters()):,}")
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ç—É–¥–µ–Ω—Ç–∞: {sum(p.numel() for p in student_model.parameters()):,}")
        
        # –î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è
        config = DistillationConfig(
            temperature=5.0,
            alpha=0.8,
            beta=0.2,
            log_interval=10,
            device='cpu'
        )
        
        distiller = KnowledgeDistillation(config)
        optimizer = optim.AdamW(student_model.parameters(), lr=5e-5)
        
        def lm_loss_fn(logits, targets):
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = targets[..., 1:].contiguous()
            return nn.CrossEntropyLoss()(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        
        print("\n–ù–∞—á–∞–ª–æ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ø–æ–∫–µ–º–æ–Ω–∞—Ö...")
        
        trained_student = distiller.apply(
            student_model=student_model,
            teacher_model=teacher_model,
            train_loader=dataloader,
            optimizer=optimizer,
            num_epochs=1,
            student_loss_fn=lm_loss_fn
        )
        
        print("‚úÖ –î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ø–æ–∫–µ–º–æ–Ω–∞—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å –ø–æ–∫–µ–º–æ–Ω–∞–º–∏: {e}")
        return False


def test_factory_function():
    """–¢–µ—Å—Ç —Ñ–∞–±—Ä–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤"""
    print("\n=== –¢–µ—Å—Ç —Ñ–∞–±—Ä–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ ===")
    
    # –¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏—Å—Ç–∏–ª–ª—è—Ç–æ—Ä–∞
    config = {
        'temperature': 3.0,
        'alpha': 0.6,
        'beta': 0.4
    }
    
    distiller = create_optimizer('distillation', config)
    assert isinstance(distiller, KnowledgeDistillation)
    assert distiller.config.temperature == 3.0
    assert distiller.config.alpha == 0.6
    
    print("‚úÖ –§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    return True


def save_metrics_to_csv(metrics: Dict[str, Any], filename: str = "distillation_metrics.csv"):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ CSV —Ñ–∞–π–ª"""
    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è CSV
        data = {
            'metric': [],
            'value': []
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        if 'metrics' in metrics:
            for key, value in metrics['metrics'].items():
                data['metric'].append(key)
                data['value'].append(value)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if 'config' in metrics:
            for key, value in metrics['config'].items():
                data['metric'].append(f"config_{key}")
                data['value'].append(value)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV
        df = pd.DataFrame(data)
        df.to_csv(filename, index=False)
        print(f"–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫: {e}")


def create_visualization(metrics: Dict[str, Any], filename: str = "distillation_results.png"):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    try:
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle('–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π', fontsize=16)
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        if 'config' in metrics:
            config = metrics['config']
            ax1.bar(['Temperature', 'Alpha', 'Beta'], 
                   [config.get('temperature', 0), config.get('alpha', 0), config.get('beta', 0)])
            ax1.set_title('–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏')
            ax1.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –ü–æ—Ç–µ—Ä–∏
        if 'metrics' in metrics:
            losses = ['avg_total_loss', 'avg_distillation_loss', 'avg_student_loss']
            loss_values = [metrics['metrics'].get(loss, 0) for loss in losses]
            ax2.bar(['Total', 'Distillation', 'Student'], loss_values)
            ax2.set_title('–°—Ä–µ–¥–Ω–∏–µ –ø–æ—Ç–µ—Ä–∏')
            ax2.set_ylabel('–ü–æ—Ç–µ—Ä—è')
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        if 'config' in metrics and 'teacher_params' in metrics['config']:
            params = [metrics['config']['teacher_params'], metrics['config']['student_params']]
            ax3.bar(['Teacher', 'Student'], params)
            ax3.set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤')
            ax3.set_ylabel('–ü–∞—Ä–∞–º–µ—Ç—Ä—ã')
            ax3.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è
        if 'config' in metrics and 'compression_ratio' in metrics['config']:
            ax4.bar(['Compression Ratio'], [metrics['config']['compression_ratio']])
            ax4.set_title('–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è')
            ax4.set_ylabel('–†–∞–∑')
        
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filename}")
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")


def run_comprehensive_test():
    """–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π"""
    print("üöÄ –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ –º–æ–¥—É–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏")
    print("=" * 60)
    
    results = {
        'basic_functionality': False,
        'training_process': False,
        'factory_function': False,
        'gpt2_distillation': False,
        'pokemon_dataset': False
    }
    
    # –ë–∞–∑–æ–≤—ã–µ —Ç–µ—Å—Ç—ã
    try:
        results['basic_functionality'] = test_basic_functionality()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞–∑–æ–≤–æ–º —Ç–µ—Å—Ç–µ: {e}")
    
    try:
        results['training_process'] = test_training_process()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ –æ–±—É—á–µ–Ω–∏—è: {e}")
    
    try:
        results['factory_function'] = test_factory_function()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ —Ñ–∞–±—Ä–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏: {e}")
    
    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Å—Ç—ã (–º–æ–≥—É—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –∏ –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)
    try:
        results['gpt2_distillation'] = test_gpt2_distillation(sample_size=50)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ GPT-2: {e}")
    
    try:
        results['pokemon_dataset'] = test_pokemon_dataset(sample_size=100)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ —Å –ø–æ–∫–µ–º–æ–Ω–∞–º–∏: {e}")
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print("\n" + "=" * 60)
    print("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print("=" * 60)
    
    passed = sum(results.values())
    total = len(results)
    
    for test_name, passed_test in results.items():
        status = "‚úÖ –ü–†–û–ô–î–ï–ù" if passed_test else "‚ùå –ù–ï –ü–†–û–ô–î–ï–ù"
        print(f"{test_name.replace('_', ' ').title()}: {status}")
    
    print(f"\n–û–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {passed}/{total} —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ")
    
    if passed == total:
        print("üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ! –ú–æ–¥—É–ª—å –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.")
    elif passed >= total * 0.6:
        print("‚ö†Ô∏è –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ. –ú–æ–¥—É–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏.")
    else:
        print("üö® –ú–Ω–æ–≥–æ —Ç–µ—Å—Ç–æ–≤ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–æ. –¢—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥—É–ª—è.")
    
    return results


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤"""
    parser = argparse.ArgumentParser(description='–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π')
    parser.add_argument('--test', choices=['basic', 'training', 'gpt2', 'pokemon', 'factory', 'all'], 
                       default='all', help='–¢–∏–ø —Ç–µ—Å—Ç–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞')
    parser.add_argument('--sample_size', type=int, default=100, 
                       help='–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤ —Å –¥–∞–Ω–Ω—ã–º–∏')
    parser.add_argument('--save_metrics', action='store_true', 
                       help='–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤ CSV')
    parser.add_argument('--create_plots', action='store_true', 
                       help='–°–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    parser.add_argument('--install_deps', action='store_true', 
                       help='–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏')
    
    args = parser.parse_args()
    
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    if args.install_deps:
        print("–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")
        os.system("pip install torch transformers datasets matplotlib pandas numpy")
        print("–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã.")
        return
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥—É–ª—è
    try:
        from distillation import KnowledgeDistillation
        print("‚úÖ –ú–æ–¥—É–ª—å distillation.py –Ω–∞–π–¥–µ–Ω –∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
    except ImportError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª—è distillation.py: {e}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª distillation.py –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
        sys.exit(1)
    
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
    if args.test == 'all':
        results = run_comprehensive_test()
    elif args.test == 'basic':
        test_basic_functionality()
    elif args.test == 'training':
        test_training_process()
    elif args.test == 'gpt2':
        test_gpt2_distillation(args.sample_size)
    elif args.test == 'pokemon':
        test_pokemon_dataset(args.sample_size)
    elif args.test == 'factory':
        test_factory_function()
    
    print("\nüèÅ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


if __name__ == "__main__":
    main()