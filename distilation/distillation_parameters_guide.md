# Документация по параметрам дистилляции

## Функция `distill_models()`

Эта функция выполняет дистилляцию знаний между двумя моделями - учителем (большая модель) и студентом (меньшая модель).

## Параметры функции

### Модели

#### `teacher_model` (str, по умолчанию: 'gpt2-xl')
**Что это:** Название или путь к модели учителя (большая модель, которая передает знания)
**Как влияет:** Определяет качество знаний, которые будут переданы студенту. Более крупные модели дают лучшие результаты, но требуют больше ресурсов
**Как менять:** 
- Для лучшего качества: 'gpt2-xl', 'Qwen/Qwen-1_8B', 'microsoft/DialoGPT-large'
- Для экономии ресурсов: 'gpt2-medium', 'gpt2'
**Примеры:** 'gpt2-xl', 'gpt2-medium', 'Qwen/Qwen-1_8B'

#### `student_model` (str, по умолчанию: 'arnir0/Tiny-LLM')
**Что это:** Название или путь к модели студента (маленькая модель, которая обучается)
**Как влияет:** Определяет размер и скорость финальной модели. Меньшие модели работают быстрее, но могут хуже качество
**Как менять:**
- Для максимальной скорости: 'arnir0/Tiny-LLM', 'distilgpt2'
- Для баланса скорости/качества: 'gpt2', 'gpt2-medium'
**Примеры:** 'gpt2', 'distilgpt2', 'arnir0/Tiny-LLM'

#### `box_type` (str, по умолчанию: 'white')
**Что это:** Тип дистилляции - white-box (с доступом к внутренним логитам) или black-box (только выходы)
**Как влияет:** 
- 'white': Более эффективная дистилляция, но требует совместимости архитектур
- 'black': Менее эффективная, но работает с любыми моделями
**Как менять:** Используйте 'white' для похожих архитектур, 'black' для разных
**Варианты:** 'white', 'black'

#### `teacher_model_path` (str, по умолчанию: None)
**Что это:** Путь к локально сохраненной модели учителя
**Как влияет:** Позволяет использовать кастомные или предварительно обученные модели
**Как менять:** Укажите путь к папке с моделью или None для загрузки из HuggingFace
**Пример:** '/path/to/my/teacher/model' или None

#### `student_model_path` (str, по умолчанию: None)
**Что это:** Путь к локально сохраненной модели студента
**Как влияет:** Позволяет использовать кастомные модели студента
**Как менять:** Укажите путь к папке с моделью или None для загрузки из HuggingFace
**Пример:** '/path/to/my/student/model' или None

### Данные

#### `dataset` (str, по умолчанию: 'wikitext')
**Что это:** Название датасета для обучения
**Как влияет:** Определяет тип текстов, на которых будет обучаться модель
**Как менять:** 
- 'wikitext' - для общих текстов
- 'pokemon' - для специфичных данных
- Можете указать свой датасет через dataset_path
**Варианты:** 'wikitext', 'pokemon', или путь к файлу

#### `dataset_path` (str, по умолчанию: None)
**Что это:** Путь к пользовательскому датасету
**Как влияет:** Позволяет использовать собственные данные для дистилляции
**Как менять:** Укажите путь к файлу с текстами или None для стандартных датасетов
**Пример:** '/path/to/my/dataset.txt' или None

#### `sample_size` (int, по умолчанию: 1000)
**Что это:** Количество примеров из датасета для обучения
**Как влияет:** 
- Больше примеров = лучше качество, но дольше обучение
- Меньше примеров = быстрее, но хуже качество
**Как менять:**
- Для быстрого тестирования: 100-500
- Для хорошего качества: 1000-5000
- Для максимального качества: 10000+
**Диапазон:** 100-50000

#### `max_length` (int, по умолчанию: 256)
**Что это:** Максимальная длина текстовой последовательности в токенах
**Как влияет:** 
- Больше длина = больше контекста, но больше памяти
- Меньше длина = меньше памяти, но меньше контекста
**Как менять:**
- Для коротких текстов: 128-256
- Для длинных текстов: 512-1024
- Для очень длинных: 2048+
**Диапазон:** 64-4096

### Параметры дистилляции

#### `temperature` (float, по умолчанию: 2.0)
**Что это:** Температура для смягчения распределения вероятностей
**Как влияет:** 
- Выше температура = более мягкие распределения, лучше передача знаний
- Ниже температура = более резкие распределения, хуже передача знаний
**Как менять:**
- Для лучшей дистилляции: 3.0-5.0
- Для баланса: 2.0-3.0
- Для фокуса на точности: 1.0-2.0
**Диапазон:** 1.0-10.0

#### `alpha` (float, по умолчанию: 0.7)
**Что это:** Вес для soft targets (знания от учителя)
**Как влияет:** 
- Выше alpha = больше фокуса на имитации учителя
- Ниже alpha = больше фокуса на правильных ответах
**Как менять:**
- Для максимальной дистилляции: 0.8-0.9
- Для баланса: 0.6-0.8
- Для фокуса на точности: 0.3-0.6
**Диапазон:** 0.0-1.0 (alpha + beta должно равняться 1.0)

#### `beta` (float, по умолчанию: 0.3)
**Что это:** Вес для hard targets (правильные ответы)
**Как влияет:** 
- Выше beta = больше фокуса на правильных ответах
- Ниже beta = больше фокуса на имитации учителя
**Как менять:**
- Для максимальной точности: 0.4-0.7
- Для баланса: 0.2-0.4
- Для максимальной дистилляции: 0.1-0.2
**Диапазон:** 0.0-1.0 (alpha + beta должно равняться 1.0)

### Параметры обучения

#### `epochs` (int, по умолчанию: 2)
**Что это:** Количество полных проходов через весь датасет
**Как влияет:** 
- Больше эпох = лучше обучение, но дольше время и риск переобучения
- Меньше эпох = быстрее, но хуже качество
**Как менять:**
- Для быстрого тестирования: 1-2
- Для хорошего качества: 3-5
- Для максимального качества: 5-10
**Диапазон:** 1-20

#### `batch_size` (int, по умолчанию: 8)
**Что это:** Количество примеров, обрабатываемых одновременно
**Как влияет:** 
- Больше batch_size = стабильнее обучение, но больше памяти
- Меньше batch_size = меньше памяти, но менее стабильное обучение
**Как менять:**
- Для слабых GPU: 2-4
- Для средних GPU: 8-16
- Для мощных GPU: 32-64
**Диапазон:** 1-128

#### `learning_rate` (float, по умолчанию: 5e-5)
**Что это:** Скорость обновления весов модели
**Как влияет:** 
- Выше LR = быстрее обучение, но риск нестабильности
- Ниже LR = стабильнее, но медленнее обучение
**Как менять:**
- Для быстрого обучения: 1e-4 - 5e-4
- Для стабильного обучения: 1e-5 - 5e-5
- Для точной настройки: 1e-6 - 1e-5
**Диапазон:** 1e-6 - 1e-3

#### `no_cuda` (bool, по умолчанию: False)
**Что это:** Принудительное использование CPU вместо GPU
**Как влияет:** 
- True = медленнее обучение, но работает без GPU
- False = быстрее обучение с GPU (если доступен)
**Как менять:** Установите True, если нет GPU или есть проблемы с CUDA
**Варианты:** True, False

### Сохранение и вывод

#### `output_dir` (str, по умолчанию: './output')
**Что это:** Директория для сохранения всех результатов
**Как влияет:** Определяет, где будут сохранены модели, метрики и графики
**Как менять:** Укажите любой существующий путь
**Пример:** './my_results', '/home/user/distillation_output'

#### `folder_name` (str, по умолчанию: None)
**Что это:** Название конкретной папки для этого эксперимента
**Как влияет:** Позволяет организовать результаты разных экспериментов
**Как менять:** Укажите описательное имя или None для автоматического имени
**Пример:** 'gpt2_to_tiny_experiment_1', None

#### `checkpoint_interval` (int, по умолчанию: 50)
**Что это:** Интервал сохранения промежуточных результатов (в шагах)
**Как влияет:** 
- Меньше интервал = чаще сохранение, больше места на диске
- Больше интервал = реже сохранение, меньше места
**Как менять:**
- Для частого сохранения: 10-25
- Для редкого сохранения: 100-200
**Диапазон:** 1-1000

#### `save_metrics` (bool, по умолчанию: True)
**Что это:** Сохранять ли метрики обучения в JSON файл
**Как влияет:** Позволяет анализировать результаты после обучения
**Как менять:** False, если не нужны метрики
**Варианты:** True, False

#### `save_plot` (bool, по умолчанию: True)
**Что это:** Создавать ли графики потерь и метрик
**Как влияет:** Помогает визуально оценить процесс обучения
**Как менять:** False, если не нужны графики
**Варианты:** True, False

#### `save_best_model` (bool, по умолчанию: True)
**Что это:** Сохранять ли лучшую модель по валидационной выборке
**Как влияет:** Позволяет получить оптимальную модель
**Как менять:** False, если нужна только финальная модель
**Варианты:** True, False

### Дополнительные опции

#### `skip_validation` (bool, по умолчанию: False)
**Что это:** Пропускать ли валидацию улучшения студента
**Как влияет:** 
- True = быстрее обучение, но нет контроля качества
- False = медленнее, но с контролем качества
**Как менять:** True для быстрого тестирования
**Варианты:** True, False

#### `install_deps` (bool, по умолчанию: True)
**Что это:** Автоматически устанавливать зависимости
**Как влияет:** Обеспечивает наличие всех необходимых библиотек
**Как менять:** False, если зависимости уже установлены
**Варианты:** True, False

#### `run_tests` (bool, по умолчанию: False)
**Что это:** Запускать ли тесты перед дистилляцией
**Как влияет:** Проверяет работоспособность системы
**Как менять:** True для проверки системы
**Варианты:** True, False

#### `skip_demo` (bool, по умолчанию: True)
**Что это:** Пропускать ли демонстрацию результатов
**Как влияет:** 
- True = быстрее завершение
- False = показывает примеры работы модели
**Как менять:** False для просмотра результатов
**Варианты:** True, False

## Рекомендации по настройке

### Для быстрого тестирования:
```python
distill_models(
    sample_size=100,
    epochs=1,
    batch_size=4,
    skip_validation=True
)
```

### Для качественной дистилляции:
```python
distill_models(
    sample_size=5000,
    epochs=5,
    temperature=3.0,
    alpha=0.8,
    beta=0.2
)
```

### Для слабых компьютеров:
```python
distill_models(
    batch_size=2,
    no_cuda=True,
    sample_size=500,
    max_length=128
)
```

### Для мощных GPU:
```python
distill_models(
    batch_size=32,
    sample_size=10000,
    max_length=512,
    epochs=10
)
```